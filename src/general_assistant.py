"""
é€šç”¨åŠ©æ‰‹æ¨¡å—
å¤„ç†ä¸ä½¿ç”¨RAGçš„é€šç”¨é—®ç­”é€»è¾‘
"""
import streamlit as st
import logging
from typing import Tuple, Optional
from src.llm import get_llm_service
from prompt import get_general_assistant_prompt

logger = logging.getLogger(__name__)


def handle_general_assistant(
    prompt: str,
    message_placeholder,
    thinking_placeholder: Optional[st.delta_generator.DeltaGenerator],
    show_thinking: bool = False
) -> Tuple[str, list, str]:
    """
    å¤„ç†é€šç”¨åŠ©æ‰‹æ¨¡å¼çš„é—®ç­”
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        message_placeholder: Streamlitå ä½ç¬¦ï¼Œç”¨äºæ˜¾ç¤ºå›ç­”
        thinking_placeholder: Streamlitå ä½ç¬¦ï¼Œç”¨äºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰
        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
    
    Returns:
        Tuple[str, list, str]: (full_response, src_nodes, sources_str)
            - full_response: å®Œæ•´çš„å›ç­”å†…å®¹
            - src_nodes: æ¥æºèŠ‚ç‚¹åˆ—è¡¨ï¼ˆé€šç”¨åŠ©æ‰‹ä¸ºç©ºåˆ—è¡¨ï¼‰
            - sources_str: æ¥æºå­—ç¬¦ä¸²ï¼ˆé€šç”¨åŠ©æ‰‹ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
    """
    llm_service = get_llm_service()
    
    if not llm_service.is_available():
        error_msg = "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ API å¯†é’¥ã€‚è¯·åœ¨ config/config.json ä¸­é…ç½® DEEPSEEK_API_KEYã€OPENAI_API_KEY æˆ– DASHSCOPE_API_KEYã€‚"
        st.error(error_msg)
        full_response = "æŠ±æ­‰ï¼Œæœªé…ç½® API å¯†é’¥ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚è¯·æ£€æŸ¥ config/config.json é…ç½®æ–‡ä»¶ã€‚"
        message_placeholder.markdown(full_response)
        return full_response, [], ""
    
    # è·å–é€šç”¨åŠ©æ‰‹æç¤ºè¯
    try:
        system_prompt = get_general_assistant_prompt()
        # å°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æç¤ºè¯å‰
        enhanced_prompt = f"{system_prompt}\n\nç”¨æˆ·é—®é¢˜ï¼š{prompt}"
    except Exception as e:
        logger.warning(f"è·å–é€šç”¨åŠ©æ‰‹æç¤ºè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æç¤ºè¯")
        enhanced_prompt = prompt
    
    # ä½¿ç”¨LLMæœåŠ¡è¿›è¡Œæµå¼è°ƒç”¨
    full_response = ""
    thinking_content_final = ""
    
    try:
        # æµå¼è°ƒç”¨
        stream_success = False
        for chunk in llm_service.stream_chat(enhanced_prompt, show_thinking=show_thinking):
            if chunk["type"] == "error":
                st.error(chunk["content"])
                full_response = "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
                break
            elif chunk["type"] == "thinking":
                # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                if thinking_placeholder:
                    thinking_placeholder.markdown(f"ğŸ’­ **æ€è€ƒè¿‡ç¨‹ï¼š**\n\n{chunk['content']}â–Œ")
                thinking_content_final = chunk["content"]
            elif chunk["type"] == "content":
                # æ˜¾ç¤ºå›ç­”å†…å®¹
                if chunk.get("thinking"):
                    # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†ç¦»æ˜¾ç¤º
                    if thinking_placeholder:
                        thinking_placeholder.markdown(f"ğŸ’­ **æ€è€ƒè¿‡ç¨‹ï¼š**\n\n{chunk['thinking']}â–Œ")
                    thinking_content_final = chunk["thinking"]
                message_placeholder.markdown(chunk["content"] + "â–Œ")
                full_response = chunk["content"]
            elif chunk["type"] == "done":
                # å®Œæˆï¼Œæœ€ç»ˆå¤„ç†
                full_response = chunk["content"]
                thinking_content_final = chunk.get("thinking", "")
                
                # æ¸…é™¤æµå¼è¾“å‡ºæ—¶çš„thinking_placeholder
                if thinking_placeholder:
                    thinking_placeholder.empty()
                    thinking_placeholder = None
                
                # æ˜¾ç¤ºæœ€ç»ˆå›ç­”
                message_placeholder.markdown(full_response)
                
                # åœ¨å›ç­”å®Œæˆåï¼Œä½¿ç”¨expanderæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆé»˜è®¤æŠ˜å ï¼‰
                if show_thinking and thinking_content_final:
                    with st.expander("ğŸ’­ æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
                        st.markdown(thinking_content_final)
                stream_success = True
                break
        
        # å¦‚æœæµå¼è°ƒç”¨å¤±è´¥ï¼Œå°è¯•éæµå¼ä½œä¸ºå›é€€
        if not stream_success and not full_response:
            result = llm_service.chat(enhanced_prompt, show_thinking=show_thinking)
            if result["success"]:
                full_response = result["content"]
                thinking_content_final = result.get("thinking", "")
                
                # æ¸…é™¤æµå¼è¾“å‡ºæ—¶çš„thinking_placeholder
                if thinking_placeholder:
                    thinking_placeholder.empty()
                    thinking_placeholder = None
                
                message_placeholder.markdown(full_response)
                
                # åœ¨å›ç­”å®Œæˆåï¼Œä½¿ç”¨expanderæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆé»˜è®¤æŠ˜å ï¼‰
                if show_thinking and thinking_content_final:
                    with st.expander("ğŸ’­ æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
                        st.markdown(thinking_content_final)
            else:
                st.error(result.get("error", "ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯"))
                full_response = "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
                message_placeholder.markdown(full_response)
    
    except Exception as e:
        logger.error(f"è°ƒç”¨LLMæœåŠ¡å¤±è´¥: {e}")
        st.error(f"è°ƒç”¨LLMæœåŠ¡å¤±è´¥: {e}")
        full_response = "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
        message_placeholder.markdown(full_response)
    
    return full_response, [], ""

