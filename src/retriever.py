import os
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# --------------------------------------------------------------------------
# [é‡è¦] åœ¨å¯¼å…¥ llama_index ä¹‹å‰é…ç½® NLTK æ•°æ®è·¯å¾„
# é¿å…çº¿ä¸Šéƒ¨ç½²æ—¶çš„æƒé™é”™è¯¯ï¼ˆPermissionErrorï¼‰
# --------------------------------------------------------------------------
def _setup_nltk_data():
    """é…ç½® NLTK æ•°æ®è·¯å¾„ï¼Œç¡®ä¿ä½¿ç”¨é¡¹ç›®å†…çš„æ•°æ®"""
    try:
        import nltk
        # æ„é€ é¡¹ç›®å†…éƒ¨çš„ nltk_data æ–‡ä»¶å¤¹è·¯å¾„
        nltk_data_dir = project_root / "nltk_data"
        nltk_data_dir_str = str(nltk_data_dir)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ NLTK_DATAï¼Œç¡®ä¿ NLTK å’Œ llama_index éƒ½ä½¿ç”¨é¡¹ç›®ç›®å½•
        # è¿™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼Œå¯ä»¥é¿å…æƒé™é”™è¯¯
        if "NLTK_DATA" not in os.environ:
            os.environ["NLTK_DATA"] = nltk_data_dir_str
        
        # å°†é¡¹ç›®å†…éƒ¨çš„ nltk_data è·¯å¾„æ·»åŠ åˆ° NLTK çš„æœç´¢åˆ—è¡¨çš„æœ€å‰é¢
        # è¿™æ · NLTK ä¼šä¼˜å…ˆä½¿ç”¨é¡¹ç›®å†…çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å°è¯•ä¸‹è½½åˆ°ç³»ç»Ÿç›®å½•
        if nltk_data_dir_str not in nltk.data.path:
            # æ’å…¥åˆ°æœ€å‰é¢ï¼Œç¡®ä¿ä¼˜å…ˆä½¿ç”¨
            nltk.data.path.insert(0, nltk_data_dir_str)
        
        # æ£€æŸ¥å¹¶é¢„å…ˆä¸‹è½½å¿…è¦çš„æ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨ä¸”å¯ä»¥ä¸‹è½½ï¼‰
        # è¿™æ ·å¯ä»¥é¿å… llama_index åœ¨åˆå§‹åŒ–æ—¶å°è¯•ä¸‹è½½åˆ°ç³»ç»Ÿç›®å½•
        try:
            # æ£€æŸ¥ punkt æ•°æ®æ˜¯å¦å­˜åœ¨
            punkt_path = nltk_data_dir / "tokenizers" / "punkt"
            punkt_tab_path = nltk_data_dir / "tokenizers" / "punkt_tab"
            
            # å¦‚æœ punkt æˆ– punkt_tab éƒ½ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½
            if not punkt_path.exists() and not punkt_tab_path.exists():
                try:
                    # å°è¯•ä¸‹è½½ punkt_tabï¼ˆæ–°ç‰ˆæœ¬ï¼‰æˆ– punktï¼ˆæ—§ç‰ˆæœ¬ï¼‰
                    logging.info("æ­£åœ¨ä¸‹è½½ NLTK punkt æ•°æ®åˆ°é¡¹ç›®ç›®å½•...")
                    nltk.download("punkt_tab", download_dir=nltk_data_dir_str, quiet=True)
                except Exception as download_error:
                    # å¦‚æœ punkt_tab ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä¸‹è½½ punkt
                    try:
                        nltk.download("punkt", download_dir=nltk_data_dir_str, quiet=True)
                    except Exception:
                        # ä¸‹è½½å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œllama_index å¯èƒ½ä¼šå¤„ç†
                        logging.warning(f"NLTK æ•°æ®ä¸‹è½½å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {download_error}")
        except Exception as check_error:
            # æ£€æŸ¥æˆ–ä¸‹è½½å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            logging.debug(f"NLTK æ•°æ®æ£€æŸ¥å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {check_error}")
            
    except ImportError:
        # NLTK æœªå®‰è£…ï¼Œè·³è¿‡é…ç½®ï¼ˆæŸäº›ç¯å¢ƒå¯èƒ½ä¸éœ€è¦ NLTKï¼‰
        pass
    except Exception as e:
        # é…ç½®å¤±è´¥ä¸åº”è¯¥å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•è­¦å‘Š
        logging.warning(f"NLTK è·¯å¾„é…ç½®å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

# æ‰§è¡Œ NLTK é…ç½®
_setup_nltk_data()
# --------------------------------------------------------------------------

# æ£€æŸ¥ NumPy ç‰ˆæœ¬ï¼Œé¿å…ä¸ llama-index çš„ OpenAILike å†²çª
def _check_numpy_version():
    """æ£€æŸ¥ NumPy ç‰ˆæœ¬ï¼Œå¦‚æœç‰ˆæœ¬è¿‡é«˜åˆ™æç¤ºç”¨æˆ·"""
    try:
        import numpy as np
        numpy_version = np.__version__
        # æ£€æŸ¥æ˜¯å¦æ˜¯ 2.0 æˆ–æ›´é«˜ç‰ˆæœ¬
        major_version = int(numpy_version.split('.')[0])
        if major_version >= 2:
            logging.warning(
                f"æ£€æµ‹åˆ° NumPy {numpy_version}ï¼Œå¯èƒ½ä¸ llama-index çš„ OpenAILike å†²çªã€‚"
                f"å»ºè®®é™çº§: pip install 'numpy<2'"
            )
            return False
        return True
    except ImportError:
        # NumPy æœªå®‰è£…ï¼Œä¸å½±å“
        return True
    except Exception:
        # ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
        return True

# åœ¨æ¨¡å—åŠ è½½æ—¶æ£€æŸ¥ NumPy ç‰ˆæœ¬
_check_numpy_version()

from llama_index.core import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
    PromptTemplate,
)
try:
    from llama_index.embeddings.dashscope import (
        DashScopeEmbedding,
        DashScopeTextEmbeddingModels,
    )
except ModuleNotFoundError:
    DashScopeEmbedding = None
    DashScopeTextEmbeddingModels = None

# å°è¯•å¯¼å…¥ Chroma å‘é‡å­˜å‚¨
try:
    # åœ¨å¯¼å…¥ chromadb ä¹‹å‰ç¦ç”¨é¥æµ‹ï¼Œé¿å… posthog ç‰ˆæœ¬å…¼å®¹æ€§é”™è¯¯
    os.environ["ANONYMIZED_TELEMETRY"] = "False"
    
    import chromadb
    from llama_index.vector_stores.chroma import ChromaVectorStore
    CHROMA_AVAILABLE = True
    
    # é…ç½®æ—¥å¿—è¿‡æ»¤å™¨ï¼ŒæŠ‘åˆ¶ ChromaDB é¥æµ‹é”™è¯¯
    import logging
    chroma_logger = logging.getLogger("chromadb.telemetry")
    chroma_logger.setLevel(logging.CRITICAL)  # åªæ˜¾ç¤ºä¸¥é‡é”™è¯¯
    chroma_logger.propagate = False  # ä¸ä¼ æ’­åˆ°æ ¹æ—¥å¿—è®°å½•å™¨
except ImportError:
    CHROMA_AVAILABLE = False
    chromadb = None
    ChromaVectorStore = None

from llama_index.llms.openai import OpenAI

# å»¶è¿Ÿå¯¼å…¥ OpenAILikeï¼Œé¿å… NumPy ç‰ˆæœ¬å†²çª
def _get_openai_like():
    """
    å»¶è¿Ÿå¯¼å…¥ OpenAILikeï¼Œé¿å…å¯¼å…¥æ—¶çš„ä¾èµ–å†²çª
    
    Returns:
        OpenAILike ç±»æˆ– Noneï¼ˆå¦‚æœå¯¼å…¥å¤±è´¥ï¼‰
    """
    try:
        from llama_index.llms.openai_like import OpenAILike
        return OpenAILike
    except ImportError as e:
        error_detail = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯ NumPy ç‰ˆæœ¬é—®é¢˜
        if "numpy" in error_detail.lower() or "numpy" in str(type(e)).lower():
            logging.error(f"OpenAILike å¯¼å…¥å¤±è´¥ï¼ˆNumPy ç‰ˆæœ¬å†²çªï¼‰: {e}")
            logging.error("è§£å†³æ–¹æ¡ˆ: pip install 'numpy<2'")
        else:
            logging.warning(f"æ— æ³•å¯¼å…¥ OpenAILike: {e}")
        return None
    except (ModuleNotFoundError, RuntimeError) as e:
        logging.warning(f"æ— æ³•å¯¼å…¥ OpenAILike: {e}")
        return None
    except Exception as e:
        # æ•è·å…¶ä»–å¯èƒ½çš„é”™è¯¯ï¼ˆå¦‚ NumPy ç‰ˆæœ¬å†²çªå¯¼è‡´çš„è¿è¡Œæ—¶é”™è¯¯ï¼‰
        error_detail = str(e)
        if "numpy" in error_detail.lower():
            logging.error(f"OpenAILike å¯¼å…¥å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ NumPy ç‰ˆæœ¬å†²çªï¼‰: {e}")
            logging.error("è§£å†³æ–¹æ¡ˆ: pip install 'numpy<2'")
        else:
            logging.warning(f"æ— æ³•å¯¼å…¥ OpenAILikeï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
        return None
from src.feedback import FeedbackStore
from config.load_key import load_config, get_api_key, get_model_config, get_available_llm, load_key
try:
    from prompt import get_industry_assistant_prompt
except ImportError:
    get_industry_assistant_prompt = None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# LangSmith callbackæ”¯æŒ
def _setup_langsmith_callback():
    """
    è®¾ç½®LangSmith callbackï¼ˆå¦‚æœå¯ç”¨ï¼‰
    
    Returns:
        callback_handler: é…ç½®å¥½çš„callback handlerï¼Œå¦‚æœLangSmithæœªå¯ç”¨åˆ™è¿”å›None
    """
    try:
        config = load_config()
        monitoring_config = config.get("monitoring", {})
        langsmith_config = monitoring_config.get("langsmith", {})
        
        if not langsmith_config.get("enabled", False):
            logging.debug("LangSmithæœªå¯ç”¨ï¼Œè·³è¿‡callbackè®¾ç½®")
            return None
        
        langsmith_api_key = get_api_key("LANGCHAIN_API_KEY")
        if not langsmith_api_key:
            logging.warning("âš ï¸ LangSmithå·²é…ç½®ä½†æœªæ‰¾åˆ°LANGCHAIN_API_KEYï¼Œè¯·åœ¨config/config.jsonä¸­é…ç½®")
            return None
        
        # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…langsmith
        try:
            from langsmith import Client
            from llama_index.core.callbacks import LlamaDebugHandler
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœè¿˜æœªè®¾ç½®ï¼‰
            if not os.getenv("LANGCHAIN_API_KEY"):
                os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
            if not os.getenv("LANGCHAIN_TRACING"):
                os.environ["LANGCHAIN_TRACING"] = "true" if langsmith_config.get("tracing", True) else "false"
            if not os.getenv("LANGCHAIN_PROJECT"):
                project_name = langsmith_config.get("project", "ai-rag-pro")
                os.environ["LANGCHAIN_PROJECT"] = project_name
            
            # åˆ›å»ºLlamaDebugHandlerï¼ˆLlamaIndexçš„LangSmithé›†æˆï¼‰
            # LlamaDebugHandlerä¼šè‡ªåŠ¨ä½¿ç”¨LANGCHAINç¯å¢ƒå˜é‡
            callback_handler = LlamaDebugHandler()
            
            project_name = langsmith_config.get("project", "ai-rag-pro")
            logging.info(f"âœ… LangSmith callbackå·²å¯ç”¨ï¼Œé¡¹ç›®: {project_name}")
            logging.info(f"ğŸ“Š æŸ¥çœ‹è¿½è¸ª: https://smith.langchain.com/projects/{project_name}")
            return callback_handler
            
        except ImportError:
            logging.warning("âš ï¸ langsmithæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install langsmith")
            logging.info("ğŸ’¡ LangSmithç”¨äºè¿½è¸ªå’Œç›‘æ§LLMè°ƒç”¨ï¼Œå®‰è£…åå¯æŸ¥çœ‹è¯¦ç»†çš„è°ƒç”¨é“¾å’Œæ€§èƒ½æŒ‡æ ‡")
            return None
        except Exception as e:
            logging.warning(f"âš ï¸ LangSmith callbackè®¾ç½®å¤±è´¥: {e}")
            return None
            
    except Exception as e:
        logging.warning(f"âš ï¸ LangSmith callbackè®¾ç½®å¼‚å¸¸: {e}")
        return None

def _apply_langsmith_settings():
    """
    åº”ç”¨LangSmithè®¾ç½®åˆ°LlamaIndexå…¨å±€Settings
    åº”è¯¥åœ¨RAGManageråˆå§‹åŒ–ä¹‹å‰è°ƒç”¨
    """
    callback_handler = _setup_langsmith_callback()
    if callback_handler:
        # å°è¯•ä½¿ç”¨æ–°çš„æ–¹å¼è®¾ç½®callback_manager
        # åœ¨æ–°ç‰ˆæœ¬çš„LlamaIndexä¸­ï¼Œå¯ä»¥ç›´æ¥è®¾ç½®handleråˆ—è¡¨
        try:
            # æ–¹æ³•1: å°è¯•ç›´æ¥è®¾ç½®callback_managerä¸ºhandleråˆ—è¡¨
            Settings.callback_manager = [callback_handler]
        except (TypeError, AttributeError):
            try:
                # æ–¹æ³•2: å°è¯•ä»callbacksæ¨¡å—å¯¼å…¥CallbackManager
                from llama_index.core.callbacks import CallbackManager
                callback_manager = CallbackManager([callback_handler])
                Settings.callback_manager = callback_manager
            except ImportError:
                # æ–¹æ³•3: å¦‚æœCallbackManagerä¸å­˜åœ¨ï¼Œå°è¯•ç›´æ¥è®¾ç½®handler
                # æŸäº›ç‰ˆæœ¬å¯èƒ½æ”¯æŒç›´æ¥è®¾ç½®handler
                try:
                    Settings.callback_manager = callback_handler
                except Exception as e:
                    logging.warning(f"âš ï¸ æ— æ³•è®¾ç½®LangSmith callback_manager: {e}")
                    logging.info("ğŸ’¡ LangSmithç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼ŒLlamaDebugHandlerå°†è‡ªåŠ¨å·¥ä½œ")
                    return
        
        logging.info("âœ… LangSmith callbackå·²åº”ç”¨åˆ°LlamaIndex Settings")

class RAGManager:
    def __init__(self, 
                 knowledge_space_dir: str = None,
                 intent_space_dir: str = None,
                 persist_dir_knowledge: str = None,
                 persist_dir_intent: str = None,
                 embed_model_name: str = None,
                 llm_model_name: str = None):
        """
        åˆå§‹åŒ–RAGç®¡ç†å™¨ï¼Œé…ç½®æ¨¡å‹å’Œè·¯å¾„ã€‚
        å¦‚æœå‚æ•°ä¸ºNoneï¼Œåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–ã€‚
        """
        # é¦–å…ˆåº”ç”¨LangSmithè®¾ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        _apply_langsmith_settings()
        
        # åŠ è½½é…ç½®
        config = load_config()
        rag_config = config.get("rag", {})
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
        self.knowledge_space_dir = knowledge_space_dir or rag_config.get("knowledge_space_dir", "./rag_source/knowledge_space")
        self.intent_space_dir = intent_space_dir or rag_config.get("intent_space_dir", "./rag_source/intent_space")
        self.persist_dir_knowledge = persist_dir_knowledge or rag_config.get("persist_dir_knowledge", "./data/storage/knowledge_space")
        self.persist_dir_intent = persist_dir_intent or rag_config.get("persist_dir_intent", "./data/storage/intent_space")
        
        # Chroma æ•°æ®åº“è·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨ Chromaï¼‰
        self.chroma_db_path = rag_config.get("chroma_db_path", "./data/chroma_db")
        self.use_chroma = CHROMA_AVAILABLE and rag_config.get("use_chroma", True)  # é»˜è®¤ä½¿ç”¨ Chroma
        
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        os.makedirs(self.knowledge_space_dir, exist_ok=True)
        os.makedirs(self.intent_space_dir, exist_ok=True)
        # æ³¨æ„ï¼špersist_dir_knowledge å’Œ persist_dir_intent å·²åºŸå¼ƒï¼Œä¸å†åˆ›å»ºç›®å½•
        # ç³»ç»Ÿç°åœ¨ä»…ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨
        
        # åˆå§‹åŒ– Chroma å®¢æˆ·ç«¯ï¼ˆç³»ç»Ÿè¦æ±‚ä½¿ç”¨å‘é‡å­˜å‚¨ï¼‰
        self.chroma_client = None
        if self.use_chroma and CHROMA_AVAILABLE:
            try:
                os.makedirs(self.chroma_db_path, exist_ok=True)
                self.chroma_client = chromadb.PersistentClient(
                    path=self.chroma_db_path,
                    settings=chromadb.Settings(
                        anonymized_telemetry=False,
                        allow_reset=True
                    )
                )
                logging.info(f"âœ… Chroma å‘é‡æ•°æ®åº“å·²åˆå§‹åŒ–: {self.chroma_db_path}")
            except Exception as e:
                error_msg = f"Chroma åˆå§‹åŒ–å¤±è´¥: {e}ã€‚ç³»ç»Ÿè¦æ±‚ä½¿ç”¨å‘é‡å­˜å‚¨ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–å®‰è£… chromadb"
                logging.error(error_msg, exc_info=True)
                raise RuntimeError(error_msg)
        elif not CHROMA_AVAILABLE:
            error_msg = "Chroma æœªå®‰è£…ã€‚ç³»ç»Ÿè¦æ±‚ä½¿ç”¨å‘é‡å­˜å‚¨ï¼Œè¯·å®‰è£…: pip install chromadb"
            logging.error(error_msg)
            raise RuntimeError(error_msg)
        
        # åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯å­˜å‚¨ï¼ˆå¿…é¡»åœ¨ LLM åˆå§‹åŒ–ä¹‹å‰ï¼‰
        self.llm_error_msg = None  # å­˜å‚¨ LLM åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
        self.embed_error_msg = None  # å­˜å‚¨åµŒå…¥æ¨¡å‹åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
        self.llm_provider = None  # ç”¨äºå­˜å‚¨ 'deepseek', 'qwen' ç­‰

        # é…ç½®å…¨å±€çš„LLMå’ŒEmbeddingæ¨¡å‹
        # ä»é…ç½®æ–‡ä»¶è·å–å¯ç”¨çš„LLM
        available_llm = get_available_llm()
        if available_llm:
            self.llm_provider = available_llm  # å­˜å‚¨æä¾›å•†åç§°
            model_config = get_model_config(available_llm)
            if model_config:
                api_key = get_api_key(model_config["api_key_env"])
                if api_key:
                    base_url = model_config["base_url"]
                    model_name = model_config["model_name"]
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ OpenAI å®˜æ–¹ API
                    is_openai_official = "api.openai.com" in base_url.lower()
                    
                    # å¦‚æœä¸æ˜¯ OpenAI å®˜æ–¹ APIï¼Œå¿…é¡»ä½¿ç”¨ OpenAILike
                    if not is_openai_official:
                        OpenAILike = _get_openai_like()
                        if OpenAILike is None:
                            error_msg = f"æ— æ³•ä½¿ç”¨ {available_llm} APIï¼šéœ€è¦ OpenAILike æ”¯æŒè‡ªå®šä¹‰ base_urlï¼Œä½†å¯¼å…¥å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ NumPy ç‰ˆæœ¬å†²çªï¼‰ã€‚è¯·é™çº§ NumPy: pip install 'numpy<2'"
                            logging.error(error_msg)
                            self.llm_error_msg = error_msg
                            self.llm = None
                        else:
                            try:
                                self.llm = OpenAILike(
                                    model=model_name,
                                    api_base=base_url,
                                    api_key=api_key,
                                    is_chat_model=True,
                                    temperature=model_config.get("temperature", 0.1),
                                )
                                logging.info(f"ä½¿ç”¨ {available_llm} API ä½œä¸º LLM (OpenAILike)")
                            except Exception as e:
                                error_msg = f"OpenAILike åˆå§‹åŒ–å¤±è´¥: {e}"
                                logging.error(error_msg)
                                self.llm_error_msg = error_msg
                                self.llm = None
                    else:
                        # OpenAI å®˜æ–¹ APIï¼Œå¯ä»¥ä½¿ç”¨ OpenAI ç±»
                        try:
                            self.llm = OpenAI(
                                model=model_name,
                                api_key=api_key,
                                base_url=base_url,
                                temperature=model_config.get("temperature", 0.1),
                            )
                            logging.info(f"ä½¿ç”¨ {available_llm} API ä½œä¸º LLM (OpenAI)")
                        except Exception as e:
                            error_msg = f"OpenAI åˆå§‹åŒ–å¤±è´¥: {e}"
                            logging.error(error_msg)
                            self.llm_error_msg = error_msg
                            self.llm = None
                else:
                    error_msg = f"æœªæ‰¾åˆ° {available_llm} çš„APIå¯†é’¥"
                    logging.warning(error_msg)
                    self.llm_error_msg = error_msg
                    self.llm = None
            else:
                error_msg = f"æœªæ‰¾åˆ° {available_llm} çš„é…ç½®"
                logging.warning(error_msg)
                self.llm_error_msg = error_msg
                self.llm = None
        else:
            error_msg = "æœªæ‰¾åˆ°å¯ç”¨çš„LLMé…ç½®ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶"
            logging.warning(error_msg)
            self.llm_error_msg = error_msg
            self.llm = None
        
        # å¦‚æœä»ç„¶æ²¡æœ‰LLMï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆDashScope/Qwenï¼‰
        if self.llm is None:
            OpenAILike = _get_openai_like()
            if OpenAILike is not None:
                dashscope_key = get_api_key("DASHSCOPE_API_KEY")
                if dashscope_key:
                    try:
                        self.llm = OpenAILike(
                            model=llm_model_name or "qwen-plus",
                            api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
                            api_key=dashscope_key,
                            is_chat_model=True,
                        )
                        logging.info("ä½¿ç”¨ DashScope (Qwen) API ä½œä¸º LLM (fallback, OpenAILike)")
                    except Exception as e:
                        error_msg = f"OpenAILike åˆå§‹åŒ–å¤±è´¥: {e}"
                        logging.warning(error_msg)
                        self.llm_error_msg = error_msg
                        self.llm = None
                else:
                    error_msg = "æœªæ‰¾åˆ° DashScope API Keyï¼Œæ— æ³•ä½¿ç”¨ fallback LLM"
                    logging.warning(error_msg)
                    self.llm_error_msg = error_msg
                    self.llm = None
            else:
                # å¦‚æœ OpenAILike ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨é OpenAI API
                error_msg = "OpenAILike ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨ DeepSeek æˆ–åƒé—® APIã€‚è¯·è¿è¡Œ: pip install 'numpy<2'"
                logging.warning(error_msg)
                self.llm_error_msg = error_msg
                self.llm = None
        
        # é…ç½®Embeddingæ¨¡å‹
        self.embed_model = None
        embedding_config = config.get("embedding", {})
        embed_provider = embedding_config.get("provider", "dashscope")
        
        if embed_provider == "dashscope":
            if DashScopeEmbedding is None or DashScopeTextEmbeddingModels is None:
                error_msg = "æœªå®‰è£… llama-index-embeddings-dashscope æ¨¡å—"
                logging.error(error_msg)
                self.embed_error_msg = f"{error_msg}ã€‚è¯·è¿è¡Œ: pip install llama-index-embeddings-dashscope"
            else:
                try:
                    # ç¡®ä¿API Keyå·²åŠ è½½åˆ°ç¯å¢ƒå˜é‡
                    load_key()
                    embed_api_key = get_api_key(embedding_config.get("api_key_env", "DASHSCOPE_API_KEY"))
                    if embed_api_key:
                        # ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®
                        os.environ["DASHSCOPE_API_KEY"] = embed_api_key
                        # åˆå§‹åŒ–DashScopeEmbeddingï¼Œæ˜¾å¼ä¼ é€’api_keyå‚æ•°
                        try:
                            self.embed_model = DashScopeEmbedding(
                                model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,
                                api_key=embed_api_key
                            )
                            logging.info("âœ… DashScope Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                        except Exception as init_e:
                            error_msg = f"DashScopeEmbedding åˆå§‹åŒ–å¤±è´¥: {str(init_e)}"
                            logging.error(error_msg, exc_info=True)
                            self.embed_error_msg = error_msg
                            self.embed_model = None
                    else:
                        error_msg = "æœªæ‰¾åˆ° DashScope API Key"
                        logging.warning(error_msg)
                        self.embed_error_msg = f"{error_msg}ã€‚è¯·æ£€æŸ¥ config/config.json ä¸­çš„ DASHSCOPE_API_KEY é…ç½®"
                except Exception as e:
                    error_msg = f"DashScopeEmbedding åˆå§‹åŒ–å¼‚å¸¸: {str(e)}"
                    logging.error(error_msg, exc_info=True)
                    self.embed_error_msg = error_msg
        else:
            error_msg = f"ä¸æ”¯æŒçš„åµŒå…¥æ¨¡å‹æä¾›å•†: {embed_provider}"
            logging.warning(error_msg)
            self.embed_error_msg = error_msg
        
        # LangSmith callbackå·²åœ¨_apply_langsmith_settings()ä¸­è®¾ç½®
        # è¿™é‡Œåªéœ€è¦ç¡®ä¿Settingsæ­£ç¡®é…ç½®
        
        Settings.llm = self.llm
        if self.embed_model is not None:
            Settings.embed_model = self.embed_model
        
        self.knowledge_index = None
        self.intent_index = None
        self.feedback_store = FeedbackStore()
        if self.embed_model is not None:
            try:
                logging.info("å¼€å§‹åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†ç©ºé—´ç´¢å¼•...")
                self.knowledge_index = self._load_or_create_index(
                    self.knowledge_space_dir, 
                    persist_dir=self.persist_dir_knowledge,
                    collection_name="knowledge_space"
                )
                logging.info("âœ… çŸ¥è¯†ç©ºé—´ç´¢å¼•åŠ è½½å®Œæˆ")
                logging.info("å¼€å§‹åŠ è½½æˆ–åˆ›å»ºæ„å›¾ç©ºé—´ç´¢å¼•...")
                self.intent_index = self._load_or_create_index(
                    self.intent_space_dir,
                    persist_dir=self.persist_dir_intent,
                    collection_name="intent_space"
                )
                logging.info("âœ… æ„å›¾ç©ºé—´ç´¢å¼•åŠ è½½å®Œæˆ")
                self.refresh_intent_index()
            except Exception as e:
                error_msg = f"ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}"
                logging.error(error_msg, exc_info=True)
                self.embed_error_msg = f"{self.embed_error_msg or ''}\nç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}"
                self.knowledge_index = None
                self.intent_index = None
        else:
            if self.embed_error_msg:
                logging.warning(f"æœªæ£€æµ‹åˆ°å¯ç”¨çš„åµŒå…¥æ¨¡å‹: {self.embed_error_msg}")
            else:
                logging.warning("æœªæ£€æµ‹åˆ°å¯ç”¨çš„åµŒå…¥æ¨¡å‹ï¼ŒRAGç´¢å¼•å·²ç¦ç”¨ã€‚å¯ç”¨RAGéœ€å®‰è£… dashscope é›†æˆåŒ…ã€‚")

    def _load_or_create_index(self, documents_dir: str, persist_dir: str = None, collection_name: str = None) -> VectorStoreIndex:
        """
        åŠ è½½æˆ–åˆ›å»ºå‘é‡ç´¢å¼•ã€‚
        ç³»ç»Ÿè¦æ±‚ä½¿ç”¨ Chroma å‘é‡æ•°æ®åº“ä½œä¸ºå­˜å‚¨æ–¹å¼ã€‚
        
        Args:
            documents_dir: æ–‡æ¡£ç›®å½•
            persist_dir: æŒä¹…åŒ–ç›®å½•ï¼ˆå·²åºŸå¼ƒï¼Œä»…ç”¨äºå…¼å®¹æ—§æ¥å£ï¼‰
            collection_name: Chroma collection åç§°ï¼ˆå¿…éœ€ï¼‰
        
        Returns:
            VectorStoreIndex: å‘é‡ç´¢å¼•å¯¹è±¡
        
        Raises:
            RuntimeError: å¦‚æœ Chroma ä¸å¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥
        """
        # ç³»ç»Ÿè¦æ±‚ä½¿ç”¨ Chroma å‘é‡æ•°æ®åº“
        if not self.use_chroma or self.chroma_client is None:
            raise RuntimeError("ç³»ç»Ÿè¦æ±‚ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨ï¼Œä½† Chroma æœªæ­£ç¡®åˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        
        if not collection_name:
            raise ValueError("collection_name å‚æ•°æ˜¯å¿…éœ€çš„")
        
        return self._load_or_create_index_chroma(documents_dir, collection_name)
    
    def _load_or_create_index_chroma(self, documents_dir: str, collection_name: str) -> VectorStoreIndex:
        """ä½¿ç”¨ Chroma å‘é‡æ•°æ®åº“åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        try:
            # è·å–æˆ–åˆ›å»º Chroma collection
            try:
                chroma_collection = self.chroma_client.get_collection(name=collection_name)
                logging.info(f"ä½¿ç”¨ç°æœ‰ Chroma collection: {collection_name} (å·²æœ‰ {chroma_collection.count()} æ¡æ•°æ®)")
            except Exception:
                # Collection ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„ï¼ˆChromaä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œå…·ä½“ç±»å‹å¯èƒ½å› ç‰ˆæœ¬è€Œå¼‚ï¼‰
                chroma_collection = self.chroma_client.create_collection(name=collection_name)
                logging.info(f"åˆ›å»ºæ–° Chroma collection: {collection_name}")
            
            # åˆ›å»º ChromaVectorStore
            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            # å¦‚æœ collection å·²æœ‰æ•°æ®ï¼Œä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•
            if chroma_collection.count() > 0:
                logging.info(f"ä» Chroma collection '{collection_name}' åŠ è½½ç´¢å¼•...")
                index = VectorStoreIndex.from_vector_store(
                    vector_store=vector_store,
                    embed_model=self.embed_model
                )
                logging.info("ç´¢å¼•åŠ è½½å®Œæˆï¼ˆChromaï¼‰ã€‚")
                return index
            
            # åˆ›å»ºæ–°ç´¢å¼•
            if not os.path.exists(documents_dir) or not os.listdir(documents_dir):
                logging.warning(f"æ–‡æ¡£ç›®å½• '{documents_dir}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•ã€‚")
                from llama_index.core.schema import Document
                documents = [Document(text="è¿™æ˜¯ä¸€ä¸ªç©ºçš„å ä½æ–‡æ¡£ã€‚")]
            else:
                logging.info(f"ä» '{documents_dir}' åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºæ–°ç´¢å¼•ï¼ˆChromaï¼‰...")
                # --- [æ ¸å¿ƒä¿®æ”¹] ---
                # æ ¹æ®collection_nameé€‰æ‹©ä¸åŒçš„æ–‡æ¡£åŠ è½½æ–¹å¼
                if collection_name == "intent_space":
                    logging.info("ä½¿ç”¨Q&Aè§£æå™¨åŠ è½½æ„å›¾ç©ºé—´æ–‡æ¡£...")
                    documents = self._load_qa_documents(documents_dir)
                else:
                    logging.info("ä½¿ç”¨é»˜è®¤è§£æå™¨åŠ è½½çŸ¥è¯†ç©ºé—´æ–‡æ¡£...")
                    reader = SimpleDirectoryReader(documents_dir)
                    documents = reader.load_data()
            
            # ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨åˆ›å»ºç´¢å¼•
            index = VectorStoreIndex.from_documents(
                documents,
                storage_context=storage_context,
                embed_model=self.embed_model
            )
            logging.info(f"åˆ›å»ºæ–°ç´¢å¼•å®Œæˆï¼Œå·²ä¿å­˜åˆ° Chroma collection '{collection_name}'")
            return index
            
        except Exception as e:
            error_msg = f"Chroma ç´¢å¼•æ“ä½œå¤±è´¥: {e}ã€‚ç³»ç»Ÿè¦æ±‚ä½¿ç”¨å‘é‡å­˜å‚¨ï¼Œè¯·æ£€æŸ¥ Chroma æ•°æ®åº“çŠ¶æ€ã€‚"
            logging.error(error_msg, exc_info=True)
            raise RuntimeError(error_msg)
    
    def _load_qa_documents(self, directory: str) -> list:
        """
        ä»ç›®å½•åŠ è½½Q&Aæ ¼å¼çš„æ–‡æ¡£ã€‚
        æ¯ä¸ªQ&Aå¯¹è¢«è§£æä¸ºä¸€ä¸ªç‹¬ç«‹çš„Documentå¯¹è±¡ï¼Œå…¶ä¸­textæ˜¯é—®é¢˜ï¼ŒmetadataåŒ…å«ç­”æ¡ˆã€‚
        """
        from llama_index.core.schema import Document
        import re
        
        qa_documents = []
        if not os.path.exists(directory):
            logging.warning(f"æ„å›¾ç©ºé—´ç›®å½•ä¸å­˜åœ¨: {directory}")
            return []

        for filename in os.listdir(directory):
            if filename.endswith(".txt"):
                filepath = os.path.join(directory, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰Q&Aå¯¹
                    # (?=\nQ:|\Z) æ˜¯ä¸€ä¸ªæ­£å‘å…ˆè¡Œæ–­è¨€ï¼Œç”¨äºå¤„ç†æ–‡ä»¶æœ«å°¾çš„æœ€åä¸€ä¸ªA
                    qa_pairs = re.findall(r'Q:\s*(.*?)\s*A:\s*(.*?)(?=\nQ:|\Z)', content, re.DOTALL)
                    
                    for q, a in qa_pairs:
                        question = q.strip()
                        answer = a.strip()
                        if not question:
                            continue
                        # æ–‡æ¡£çš„textæ˜¯é—®é¢˜ï¼Œå°†è¢«å‘é‡åŒ–
                        # ç­”æ¡ˆå­˜å‚¨åœ¨å…ƒæ•°æ®ä¸­ï¼Œä»¥ä¾¿åç»­æ£€ç´¢
                        doc = Document(
                            text=question,
                            metadata={
                                "answer": answer,
                                "file_name": filename
                            }
                        )
                        qa_documents.append(doc)
                    logging.info(f"ä» {filename} åŠ è½½äº† {len(qa_pairs)} ä¸ªQ&Aå¯¹")
                except Exception as e:
                    logging.error(f"è§£æQ&Aæ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {e}")

        if not qa_documents:
            logging.warning(f"åœ¨ '{directory}' ä¸­æœªæ‰¾åˆ°ä»»ä½•Q&Aå¯¹ï¼Œå°†åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½æ–‡æ¡£ã€‚")
            qa_documents.append(Document(text="è¿™æ˜¯ä¸€ä¸ªç©ºçš„å ä½é—®é¢˜ã€‚", metadata={"answer": "è¿™æ˜¯ä¸€ä¸ªç©ºçš„å ä½å›ç­”ã€‚"}))
            
        return qa_documents

    def _load_or_create_index_json(self, documents_dir: str, persist_dir: str) -> VectorStoreIndex:
        """
        ä½¿ç”¨ JSON æ–‡ä»¶å­˜å‚¨åŠ è½½æˆ–åˆ›å»ºç´¢å¼•ï¼ˆå·²åºŸå¼ƒï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•å·²åºŸå¼ƒï¼Œç³»ç»Ÿç°åœ¨ä»…ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨ã€‚
        ä¿ç•™æ­¤æ–¹æ³•ä»…ç”¨äºå…¼å®¹æ€§ï¼Œä¸ä¼šè¢«è°ƒç”¨ã€‚
        """
        # æ£€æŸ¥æŒä¹…åŒ–ç›®å½•æ˜¯å¦å®Œæ•´ï¼ˆéœ€è¦åŒ…å«å…³é”®æ–‡ä»¶ï¼‰
        persist_dir_exists = os.path.exists(persist_dir)
        docstore_file = os.path.join(persist_dir, "docstore.json")
        index_file = os.path.join(persist_dir, "index_store.json")
        persist_complete = persist_dir_exists and os.path.exists(docstore_file) and os.path.exists(index_file)
        
        if persist_complete:
            try:
                logging.info(f"ä» '{persist_dir}' åŠ è½½ç´¢å¼•ï¼ˆJSONï¼‰...")
                storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
                index = load_index_from_storage(storage_context, embed_model=self.embed_model)
                logging.info("ç´¢å¼•åŠ è½½å®Œæˆï¼ˆJSONï¼‰ã€‚")
                return index
            except Exception as e:
                logging.warning(f"ä»æŒä¹…åŒ–ç›®å½•åŠ è½½ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†é‡æ–°åˆ›å»ºç´¢å¼•")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ é™¤ä¸å®Œæ•´çš„æŒä¹…åŒ–ç›®å½•
                import shutil
                if os.path.exists(persist_dir):
                    try:
                        shutil.rmtree(persist_dir)
                        logging.info(f"å·²åˆ é™¤ä¸å®Œæ•´çš„æŒä¹…åŒ–ç›®å½•: {persist_dir}")
                    except Exception as rm_e:
                        logging.warning(f"åˆ é™¤æŒä¹…åŒ–ç›®å½•å¤±è´¥: {rm_e}")
        
        # åˆ›å»ºæ–°ç´¢å¼•
        if not os.path.exists(documents_dir) or not os.listdir(documents_dir):
            logging.warning(f"æ–‡æ¡£ç›®å½• '{documents_dir}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•ã€‚")
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ç©ºæ–‡æ¡£æ¥åˆå§‹åŒ–ä¸€ä¸ªç©ºç´¢å¼•
            from llama_index.core.schema import Document
            documents = [Document(text="è¿™æ˜¯ä¸€ä¸ªç©ºçš„å ä½æ–‡æ¡£ã€‚")]
        else:
            logging.info(f"ä» '{documents_dir}' åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºæ–°ç´¢å¼•ï¼ˆJSONï¼‰...")
            reader = SimpleDirectoryReader(documents_dir)
            documents = reader.load_data()
        
        index = VectorStoreIndex.from_documents(documents)
        logging.info(f"åˆ›å»ºæ–°ç´¢å¼•å®Œæˆï¼ŒæŒä¹…åŒ–åˆ° '{persist_dir}'ï¼ˆJSONï¼‰...")
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        os.makedirs(persist_dir, exist_ok=True)
        index.storage_context.persist(persist_dir=persist_dir)
        logging.info("ç´¢å¼•æŒä¹…åŒ–å®Œæˆï¼ˆJSONï¼‰ã€‚")
        return index

    def _get_industry_prompt_template(self, show_thinking: bool = False) -> PromptTemplate:
        """è·å–è¡Œä¸šåŠ©æ‰‹çš„æç¤ºè¯æ¨¡æ¿"""
        if get_industry_assistant_prompt is not None:
            try:
                industry_prompt = get_industry_assistant_prompt()
                # å°†è¡Œä¸šåŠ©æ‰‹æç¤ºè¯ä¸LlamaIndexçš„é»˜è®¤æ¨¡æ¿æ ¼å¼ç»“åˆ
                thinking_instruction = ""
                if show_thinking:
                    thinking_instruction = """

## æ€è€ƒè¿‡ç¨‹è¦æ±‚
åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·å…ˆå±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. å¦‚ä½•ç†è§£ç”¨æˆ·çš„é—®é¢˜
2. å¦‚ä½•ä»å‚è€ƒä¿¡æ¯ä¸­æå–å…³é”®ä¿¡æ¯
3. å¦‚ä½•ç»„ç»‡ç­”æ¡ˆçš„é€»è¾‘ç»“æ„

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

**æ€è€ƒè¿‡ç¨‹ï¼š**
[ä½ çš„æ€è€ƒè¿‡ç¨‹]

**å›ç­”ï¼š**
[ä½ çš„æœ€ç»ˆå›ç­”]"""
                
                prompt_template_str = f"""{industry_prompt}

## å‚è€ƒä¿¡æ¯
ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
---------------------
{{context_str}}
---------------------

## ç”¨æˆ·é—®é¢˜
{{query_str}}
{thinking_instruction}

## å›ç­”è¦æ±‚
è¯·åŸºäºä»¥ä¸Šå‚è€ƒä¿¡æ¯å’Œè¡Œä¸šåŠ©æ‰‹çš„åŸåˆ™ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰ä»·å€¼çš„å›ç­”ã€‚
å›ç­”ï¼š"""
                return PromptTemplate(prompt_template_str)
            except Exception as e:
                logging.warning(f"åŠ è½½è¡Œä¸šåŠ©æ‰‹æç¤ºè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
        
        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
        thinking_instruction = ""
        if show_thinking:
            thinking_instruction = "\n\nåœ¨å›ç­”å‰ï¼Œè¯·å…ˆå±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œæ ¼å¼ï¼š\n**æ€è€ƒè¿‡ç¨‹ï¼š**\n[æ€è€ƒå†…å®¹]\n\n**å›ç­”ï¼š**\n[å›ç­”å†…å®¹]"
        
        default_template = f"""ä½ æ˜¯è¡Œä¸šåŠ©æ‰‹ï¼Œç”±å‡¡æ¢¦æ–‡åŒ–åˆ›å»ºçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚

è¯·åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ï¼š
---------------------
{{context_str}}
---------------------

é—®é¢˜ï¼š{{query_str}}{thinking_instruction}
å›ç­”ï¼š"""
        return PromptTemplate(default_template)
    
    def _get_query_engine(self, index, index_name: str, streaming=True, 
                         similarity_top_k: int = 3, show_thinking: bool = False):
        """
        è·å–æŸ¥è¯¢å¼•æ“çš„å…¬å…±æ–¹æ³•
        
        Args:
            index: å‘é‡ç´¢å¼•å¯¹è±¡
            index_name: ç´¢å¼•åç§°ï¼ˆç”¨äºé”™è¯¯æç¤ºï¼‰
            streaming: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            similarity_top_k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        
        Returns:
            é…ç½®å¥½çš„æŸ¥è¯¢å¼•æ“
        
        Raises:
            RuntimeError: å½“ç´¢å¼•æˆ–LLMæœªåˆå§‹åŒ–æ—¶
        """
        if index is None:
            error_detail = self.embed_error_msg or "åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–æˆ–ç´¢å¼•åŠ è½½å¤±è´¥"
            raise RuntimeError(f"RAGæœªå¯ç”¨æˆ–åµŒå…¥ä¸å¯ç”¨ï¼Œæ— æ³•è·å–{index_name}å¼•æ“ã€‚åŸå› ï¼š{error_detail}")
        
        if self.llm is None:
            error_msg = "LLMæœªåˆå§‹åŒ–ã€‚"
            if hasattr(self, 'llm_error_msg') and self.llm_error_msg:
                error_msg += f" {self.llm_error_msg}"
            elif hasattr(self, 'embed_error_msg') and "OpenAILike" in str(self.embed_error_msg or ""):
                error_msg += " æ— æ³•ä½¿ç”¨é OpenAI APIï¼ˆå¦‚ DeepSeekï¼‰ï¼Œå› ä¸º OpenAILike å¯¼å…¥å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ NumPy ç‰ˆæœ¬å†²çªï¼‰ã€‚è¯·è¿è¡Œ: pip install 'numpy<2'"
            else:
                error_msg += " è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ LLM é…ç½®å’Œ API Keyã€‚"
            raise RuntimeError(error_msg)
        
        query_engine = index.as_query_engine(
            streaming=streaming, similarity_top_k=similarity_top_k
        )
        
        # åº”ç”¨è¡Œä¸šåŠ©æ‰‹æç¤ºè¯æ¨¡æ¿
        prompt_template = self._get_industry_prompt_template(show_thinking=show_thinking)
        query_engine.update_prompts(
            {"response_synthesizer:text_qa_template": prompt_template}
        )
        
        return query_engine
    
    def get_knowledge_query_engine(self, streaming=True, similarity_top_k: int = 3, show_thinking: bool = False):
        """è·å–çŸ¥è¯†ç©ºé—´çš„æŸ¥è¯¢å¼•æ“"""
        logging.info("è·å–çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å¼•æ“ã€‚")
        return self._get_query_engine(
            self.knowledge_index, 
            "çŸ¥è¯†ç©ºé—´", 
            streaming, 
            similarity_top_k, 
            show_thinking
        )
        
    def get_intent_query_engine(self, streaming=True, similarity_top_k: int = 1, show_thinking: bool = False):
        """è·å–æ„å›¾ç©ºé—´çš„æŸ¥è¯¢å¼•æ“"""
        logging.info("è·å–æ„å›¾ç©ºé—´æŸ¥è¯¢å¼•æ“ã€‚")
        return self._get_query_engine(
            self.intent_index, 
            "æ„å›¾ç©ºé—´", 
            streaming, 
            similarity_top_k, 
            show_thinking
        )

    def refresh_intent_index(self) -> None:
        """åˆ·æ–°æ„å›¾ç©ºé—´ç´¢å¼•"""
        from llama_index.core import VectorStoreIndex
        if self.embed_model is None:
            logging.warning("åµŒå…¥ä¸å¯ç”¨ï¼Œè·³è¿‡æ„å›¾ç´¢å¼•åˆ·æ–°")
            return
        
        # --- [æ ¸å¿ƒä¿®æ”¹] ---
        # ä½¿ç”¨æ–°çš„Q&Aè§£æå™¨åŠ è½½æ–‡æ¡£
        docs = self._load_qa_documents(self.intent_space_dir)
        
        # å°†åé¦ˆç©ºé—´ä¸­çš„ä¼˜è´¨æ–‡æ¡£ä¹ŸåŠ å…¥æ„å›¾ç©ºé—´
        positive_feedback_docs = self.feedback_store.get_positive_documents()
        if positive_feedback_docs:
            docs.extend(positive_feedback_docs)
            logging.info(f"ä»åé¦ˆç©ºé—´åŠ è½½äº† {len(positive_feedback_docs)} ä¸ªä¼˜è´¨å›ç­”åˆ°æ„å›¾ç©ºé—´")

        if not docs:
            logging.warning("æ²¡æœ‰å¯ç”¨äºåˆ·æ–°æ„å›¾ç´¢å¼•çš„æ–‡æ¡£ï¼Œæ“ä½œä¸­æ­¢ã€‚")
            # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©æ¸…ç©ºç´¢å¼•æˆ–ä¿æŒåŸæ ·ã€‚è¿™é‡Œé€‰æ‹©ä¿æŒåŸæ ·ã€‚
            return
        
        # ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨ï¼ˆç³»ç»Ÿè¦æ±‚ï¼‰
        if not self.use_chroma or self.chroma_client is None:
            raise RuntimeError("ç³»ç»Ÿè¦æ±‚ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨ï¼Œä½† Chroma æœªæ­£ç¡®åˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        
        try:
            # åˆ é™¤ç°æœ‰ collectionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                self.chroma_client.delete_collection(name="intent_space")
                logging.info("å·²åˆ é™¤ç°æœ‰ intent_space collection")
            except Exception:
                # Collection ä¸å­˜åœ¨ï¼Œç»§ç»­åˆ›å»º
                pass
            
            # åˆ›å»ºæ–°çš„ collection
            chroma_collection = self.chroma_client.create_collection(name="intent_space")
            
            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            self.intent_index = VectorStoreIndex.from_documents(
                docs,
                storage_context=storage_context,
                embed_model=self.embed_model
            )
            logging.info("æ„å›¾ç©ºé—´ç´¢å¼•å·²åˆ·æ–°ï¼ˆChromaï¼‰")
        except Exception as e:
            error_msg = f"Chroma åˆ·æ–°å¤±è´¥: {e}ã€‚ç³»ç»Ÿè¦æ±‚ä½¿ç”¨å‘é‡å­˜å‚¨ï¼Œè¯·æ£€æŸ¥ Chroma æ•°æ®åº“çŠ¶æ€ã€‚"
            logging.error(error_msg, exc_info=True)
            raise RuntimeError(error_msg)

    def refresh_knowledge_index(self) -> None:
        """åˆ·æ–°çŸ¥è¯†ç©ºé—´ç´¢å¼•"""
        from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
        if self.embed_model is None:
            logging.warning("åµŒå…¥ä¸å¯ç”¨ï¼Œè·³è¿‡çŸ¥è¯†ç´¢å¼•åˆ·æ–°")
            return
        
        docs = []
        if os.path.exists(self.knowledge_space_dir) and os.listdir(self.knowledge_space_dir):
            reader = SimpleDirectoryReader(self.knowledge_space_dir)
            docs.extend(reader.load_data())
        if len(docs) == 0:
            from llama_index.core.schema import Document
            docs = [Document(text="")]
        
        # ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨ï¼ˆç³»ç»Ÿè¦æ±‚ï¼‰
        if not self.use_chroma or self.chroma_client is None:
            raise RuntimeError("ç³»ç»Ÿè¦æ±‚ä½¿ç”¨ Chroma å‘é‡å­˜å‚¨ï¼Œä½† Chroma æœªæ­£ç¡®åˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        
        try:
            # åˆ é™¤ç°æœ‰ collectionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                self.chroma_client.delete_collection(name="knowledge_space")
                logging.info("å·²åˆ é™¤ç°æœ‰ knowledge_space collection")
            except Exception:
                # Collection ä¸å­˜åœ¨ï¼Œç»§ç»­åˆ›å»º
                pass
            
            # åˆ›å»ºæ–°çš„ collection
            chroma_collection = self.chroma_client.create_collection(name="knowledge_space")
            
            vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            self.knowledge_index = VectorStoreIndex.from_documents(
                docs,
                storage_context=storage_context,
                embed_model=self.embed_model
            )
            logging.info("çŸ¥è¯†ç©ºé—´ç´¢å¼•å·²åˆ·æ–°ï¼ˆChromaï¼‰")
        except Exception as e:
            error_msg = f"Chroma åˆ·æ–°å¤±è´¥: {e}ã€‚ç³»ç»Ÿè¦æ±‚ä½¿ç”¨å‘é‡å­˜å‚¨ï¼Œè¯·æ£€æŸ¥ Chroma æ•°æ®åº“çŠ¶æ€ã€‚"
            logging.error(error_msg, exc_info=True)
            raise RuntimeError(error_msg)

