"""
è¡Œä¸šåŠ©æ‰‹æ¨¡å—
å¤„ç†ä½¿ç”¨RAGçš„è¡Œä¸šé—®ç­”é€»è¾‘ï¼ŒåŒ…æ‹¬æ„å›¾ç©ºé—´å’ŒçŸ¥è¯†ç©ºé—´æŸ¥è¯¢
"""
import streamlit as st
import logging
import inspect
from typing import Tuple, Optional, Any
from src.retriever import RAGManager

logger = logging.getLogger(__name__)


def _separate_thinking_and_answer(
    content: str,
    message_placeholder,
    thinking_placeholder: Optional[st.delta_generator.DeltaGenerator],
    show_thinking: bool
) -> Tuple[str, str]:
    """
    åˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œå›ç­”å†…å®¹
    
    Returns:
        Tuple[str, str]: (answer_part, thinking_part)
    """
    if show_thinking and "**å›ç­”ï¼š**" in content:
        parts = content.split("**å›ç­”ï¼š**", 1)
        if len(parts) == 2:
            thinking_part = parts[0].replace("**æ€è€ƒè¿‡ç¨‹ï¼š**", "").strip()
            answer_part = parts[1].strip()
            return answer_part, thinking_part
    
    return content, ""


def _handle_streaming_response(
    response_stream: Any,
    message_placeholder,
    thinking_placeholder: Optional[st.delta_generator.DeltaGenerator],
    show_thinking: bool
) -> Tuple[str, str]:
    """
    å¤„ç†æµå¼å“åº”
    
    Returns:
        Tuple[str, str]: (full_response, thinking_content_final)
    """
    full_response = ""
    thinking_content_final = ""
    
    if hasattr(response_stream, 'response_gen'):
        # æµå¼å“åº”
        for token in response_stream.response_gen:
            full_response += token
            
            # å¦‚æœå¯ç”¨äº†æ€è€ƒè¿‡ç¨‹ï¼Œåœ¨æµå¼è¾“å‡ºæ—¶åˆ†ç¦»æ˜¾ç¤º
            if show_thinking and "**æ€è€ƒè¿‡ç¨‹ï¼š**" in full_response:
                if "**å›ç­”ï¼š**" in full_response:
                    # å·²ç»åŒ…å«å›ç­”éƒ¨åˆ†ï¼Œåˆ†ç¦»æ˜¾ç¤º
                    answer_part, thinking_part = _separate_thinking_and_answer(
                        full_response, message_placeholder, thinking_placeholder, show_thinking
                    )
                    if thinking_placeholder and thinking_part:
                        thinking_placeholder.markdown(f"ğŸ’­ **æ€è€ƒè¿‡ç¨‹ï¼š**\n\n{thinking_part}â–Œ")
                    message_placeholder.markdown(answer_part + "â–Œ")
                else:
                    # è¿˜åœ¨æ€è€ƒé˜¶æ®µ
                    thinking_part = full_response.replace("**æ€è€ƒè¿‡ç¨‹ï¼š**", "").strip()
                    if thinking_placeholder:
                        thinking_placeholder.markdown(f"ğŸ’­ **æ€è€ƒè¿‡ç¨‹ï¼š**\n\n{thinking_part}â–Œ")
            else:
                # æ²¡æœ‰æ€è€ƒè¿‡ç¨‹æ ‡è®°ï¼Œç›´æ¥æ˜¾ç¤º
                message_placeholder.markdown(full_response + "â–Œ")
        
        # æœ€ç»ˆå¤„ç†ï¼šåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œå›ç­”
        if show_thinking and "**å›ç­”ï¼š**" in full_response:
            answer_part, thinking_part = _separate_thinking_and_answer(
                full_response, message_placeholder, thinking_placeholder, show_thinking
            )
            message_placeholder.markdown(answer_part)
            full_response = answer_part
            thinking_content_final = thinking_part
        else:
            message_placeholder.markdown(full_response)
        
        # æ¸…é™¤æµå¼è¾“å‡ºæ—¶çš„thinking_placeholderï¼Œé¿å…ä¸expanderé‡å¤
        if thinking_placeholder:
            thinking_placeholder.empty()
            thinking_placeholder = None
    else:
        # éæµå¼å“åº”
        if hasattr(response_stream, "response"):
            full_response = str(response_stream.response)
        elif hasattr(response_stream, "get_response"):
            full_response = str(response_stream.get_response())
        else:
            full_response = str(response_stream)
        
        # å¤„ç†æ€è€ƒè¿‡ç¨‹å’Œå›ç­”çš„åˆ†ç¦»
        if show_thinking and "**å›ç­”ï¼š**" in full_response:
            answer_part, thinking_part = _separate_thinking_and_answer(
                full_response, message_placeholder, thinking_placeholder, show_thinking
            )
            message_placeholder.markdown(answer_part)
            full_response = answer_part
            thinking_content_final = thinking_part
        else:
            message_placeholder.markdown(full_response)
    
    return full_response, thinking_content_final


def _query_intent_space(
    rag_manager: RAGManager,
    prompt: str,
    k_intent: int,
    intent_threshold: float,
    show_thinking: bool
) -> Tuple[str, float, list]:
    """
    æŸ¥è¯¢æ„å›¾ç©ºé—´
    
    Returns:
        Tuple[str, float, list]: (intent_text, intent_score, intent_src_nodes)
    """
    intent_text = ""
    intent_score = 0.0
    intent_src_nodes = []
    
    # æ£€æŸ¥æ„å›¾ç©ºé—´ç´¢å¼•æ˜¯å¦å¯ç”¨
    if rag_manager.intent_index is None:
        logger.warning("æ„å›¾ç©ºé—´ç´¢å¼•ä¸å¯ç”¨ï¼Œè·³è¿‡æ„å›¾ç©ºé—´æŸ¥è¯¢")
        return intent_text, intent_score, intent_src_nodes
    
    try:
        intent_engine = rag_manager.get_intent_query_engine(
            streaming=False, 
            similarity_top_k=k_intent, 
            show_thinking=show_thinking
        )
        intent_response = intent_engine.query(prompt)
        
        # è·å–å“åº”æ–‡æœ¬
        if hasattr(intent_response, "response"):
            intent_text = str(intent_response.response)
        elif hasattr(intent_response, "get_response"):
            intent_text = str(intent_response.get_response())
        else:
            intent_text = str(intent_response)
        
        intent_src_nodes = getattr(intent_response, "source_nodes", [])
        if intent_src_nodes:
            top = intent_src_nodes[0]
            intent_score = getattr(top, "score", 0.0) or 0.0
    except Exception as e:
        logger.warning(f"æ„å›¾ç©ºé—´æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
        intent_text = ""
    
    return intent_text, intent_score, intent_src_nodes


def _query_knowledge_space(
    rag_manager: RAGManager,
    prompt: str,
    k_knowledge: int,
    message_placeholder,
    thinking_placeholder: Optional[st.delta_generator.DeltaGenerator],
    show_thinking: bool
) -> Tuple[str, str, list]:
    """
    æŸ¥è¯¢çŸ¥è¯†ç©ºé—´
    
    Returns:
        Tuple[str, str, list]: (full_response, thinking_content_final, src_nodes)
    """
    full_response = ""
    thinking_content_final = ""
    src_nodes = []
    
    try:
        # æ£€æŸ¥æ–¹æ³•æ˜¯å¦æ”¯æŒ show_thinking å‚æ•°
        sig = inspect.signature(rag_manager.get_knowledge_query_engine)
        if 'show_thinking' in sig.parameters:
            query_engine = rag_manager.get_knowledge_query_engine(
                streaming=True, 
                similarity_top_k=k_knowledge, 
                show_thinking=show_thinking
            )
        else:
            # æ—§ç‰ˆæœ¬ä¸æ”¯æŒ show_thinking å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è°ƒç”¨
            logger.warning("RAGManager ç‰ˆæœ¬è¾ƒæ—§ï¼Œä¸æ”¯æŒ show_thinking å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è°ƒç”¨")
            query_engine = rag_manager.get_knowledge_query_engine(
                streaming=True, 
                similarity_top_k=k_knowledge
            )
            # æ¸…é™¤ç¼“å­˜ä»¥é‡æ–°åŠ è½½æ–°ç‰ˆæœ¬
            st.cache_resource.clear()
        
        response_stream = query_engine.query(prompt)
        full_response, thinking_content_final = _handle_streaming_response(
            response_stream, message_placeholder, thinking_placeholder, show_thinking
        )
        
        # åœ¨å›ç­”å®Œæˆåï¼Œä½¿ç”¨expanderæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆé»˜è®¤æŠ˜å ï¼‰
        if show_thinking and thinking_content_final:
            with st.expander("ğŸ’­ æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
                st.markdown(thinking_content_final)
        
        src_nodes = getattr(response_stream, "source_nodes", [])
        
    except RuntimeError as e:
        # RAGæœªå¯ç”¨æˆ–åµŒå…¥ä¸å¯ç”¨çš„é”™è¯¯
        error_msg = str(e)
        if "RAGæœªå¯ç”¨" in error_msg or "åµŒå…¥ä¸å¯ç”¨" in error_msg:
            # æå–è¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_detail = error_msg
            if "åŸå› ï¼š" in error_msg:
                error_detail = error_msg.split("åŸå› ï¼š", 1)[1].strip()
            
            detailed_error = f"""
            **âŒ çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å¤±è´¥**
            
            **é”™è¯¯åŸå› ï¼š** RAGæœªå¯ç”¨æˆ–åµŒå…¥æ¨¡å‹ä¸å¯ç”¨
            
            **è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š**
            {error_detail}
            
            **è§£å†³æ­¥éª¤ï¼š**
            1. æ£€æŸ¥ `config/config.json` ä¸­çš„ DashScope API Key é…ç½®
            2. ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼š`pip install llama-index llama-index-embeddings-dashscope`
            3. éªŒè¯ API Key æ˜¯å¦æœ‰æ•ˆï¼ˆå¯ä»¥åœ¨ DashScope æ§åˆ¶å°æ£€æŸ¥ï¼‰
            4. ç¡®ä¿ `rag_source/knowledge_space` ç›®å½•ä¸­æœ‰æ–‡æ¡£æ–‡ä»¶
            5. é‡å¯åº”ç”¨ä»¥é‡æ–°åŠ è½½é…ç½®
            
            **ä¸´æ—¶æ–¹æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨"é€šç”¨åŠ©æ‰‹"æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸ä¾èµ–çŸ¥è¯†åº“ã€‚
            """
            st.error(detailed_error)
            full_response = "âš ï¸ çŸ¥è¯†ç©ºé—´æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ä½¿ç”¨é€šç”¨åŠ©æ‰‹æ¨¡å¼ã€‚"
        elif "LLMæœªåˆå§‹åŒ–" in error_msg or "LLM" in error_msg:
            # LLM åˆå§‹åŒ–å¤±è´¥çš„é”™è¯¯
            error_detail = error_msg
            if "LLMæœªåˆå§‹åŒ–" in error_msg:
                error_detail = error_msg.replace("LLMæœªåˆå§‹åŒ–ã€‚", "").strip()
            
            detailed_error = f"""
            **âŒ çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å¤±è´¥**
            
            **é”™è¯¯åŸå› ï¼š** LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æœªåˆå§‹åŒ–
            
            **è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š**
            {error_detail}
            
            **è§£å†³æ­¥éª¤ï¼š**
            1. **å¦‚æœä½¿ç”¨ DeepSeek æˆ–å…¶ä»–é OpenAI APIï¼š**
               - è¿è¡Œï¼š`pip install 'numpy<2'` è§£å†³ NumPy ç‰ˆæœ¬å†²çª
               - é‡å¯åº”ç”¨
            
            2. **å¦‚æœä½¿ç”¨ OpenAI APIï¼š**
               - æ£€æŸ¥ `config/config.json` ä¸­çš„ `OPENAI_API_KEY` é…ç½®
               - ç¡®ä¿ API Key æœ‰æ•ˆ
            
            3. **é€šç”¨è§£å†³æ–¹æ¡ˆï¼š**
               - æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ LLM é…ç½®
               - ç¡®ä¿ API Key å·²æ­£ç¡®é…ç½®
               - é‡å¯åº”ç”¨ä»¥é‡æ–°åŠ è½½é…ç½®
            
            **ä¸´æ—¶æ–¹æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨"é€šç”¨åŠ©æ‰‹"æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä¸ä¾èµ–çŸ¥è¯†åº“ã€‚
            """
            st.error(detailed_error)
            full_response = "âš ï¸ çŸ¥è¯†ç©ºé—´æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ä½¿ç”¨é€šç”¨åŠ©æ‰‹æ¨¡å¼ã€‚"
        else:
            st.error(f"çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å¤±è´¥: {error_msg}")
            full_response = "æŠ±æ­‰ï¼ŒæŸ¥è¯¢çŸ¥è¯†ç©ºé—´æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        message_placeholder.markdown(full_response)
        logger.error(f"çŸ¥è¯†ç©ºé—´æŸ¥è¯¢RuntimeError: {error_msg}", exc_info=True)
    except Exception as e:
        error_msg = f"çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        st.error(error_msg)
        full_response = "æŠ±æ­‰ï¼ŒæŸ¥è¯¢çŸ¥è¯†ç©ºé—´æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        message_placeholder.markdown(full_response)
    
    return full_response, thinking_content_final, src_nodes


def handle_industry_assistant(
    rag_manager: RAGManager,
    prompt: str,
    message_placeholder,
    thinking_placeholder: Optional[st.delta_generator.DeltaGenerator],
    k_intent: int = 1,
    k_knowledge: int = 3,
    intent_threshold: float = 0.85,
    show_thinking: bool = False
) -> Tuple[str, list, str, bool, float]:
    """
    å¤„ç†è¡Œä¸šåŠ©æ‰‹æ¨¡å¼çš„é—®ç­”
    
    Args:
        rag_manager: RAGç®¡ç†å™¨å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        message_placeholder: Streamlitå ä½ç¬¦ï¼Œç”¨äºæ˜¾ç¤ºå›ç­”
        thinking_placeholder: Streamlitå ä½ç¬¦ï¼Œç”¨äºæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰
        k_intent: æ„å›¾ç©ºé—´æ£€ç´¢æ•°é‡
        k_knowledge: çŸ¥è¯†ç©ºé—´æ£€ç´¢æ•°é‡
        intent_threshold: æ„å›¾ç©ºé—´ç›¸ä¼¼åº¦é˜ˆå€¼
        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
    
    Returns:
        Tuple[str, list, str, bool, float]: (full_response, src_nodes, sources_str, used_intent_space, intent_score)
            - full_response: å®Œæ•´å›ç­”
            - src_nodes: æºèŠ‚ç‚¹åˆ—è¡¨
            - sources_str: æ¥æºå­—ç¬¦ä¸²
            - used_intent_space: æ˜¯å¦ä½¿ç”¨äº†æ„å›¾ç©ºé—´å¿«é€ŸåŒ¹é…
            - intent_score: æ„å›¾ç©ºé—´ç›¸ä¼¼åº¦åˆ†æ•°
    """
    # æ£€æŸ¥çŸ¥è¯†ç©ºé—´æ˜¯å¦å¯ç”¨
    if rag_manager.knowledge_index is None:
        # è·å–è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_detail = ""
        if hasattr(rag_manager, 'embed_error_msg') and rag_manager.embed_error_msg:
            error_detail = f"\n\n**è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š**\n{rag_manager.embed_error_msg}"
        
        error_msg = f"""
        **âŒ çŸ¥è¯†ç©ºé—´ä¸å¯ç”¨**
        
        **å¯èƒ½çš„åŸå› ï¼š**
        1. **åµŒå…¥æ¨¡å‹æœªé…ç½®**ï¼šè¯·æ£€æŸ¥ `config/config.json` ä¸­çš„ DashScope API Key é…ç½®
        2. **ä¾èµ–åŒ…æœªå®‰è£…**ï¼šè¯·è¿è¡Œ `pip install llama-index-embeddings-dashscope`
        3. **APIå¯†é’¥æ— æ•ˆ**ï¼šè¯·ç¡®è®¤ DashScope API Key æ˜¯å¦æ­£ç¡®
        4. **ç´¢å¼•åŠ è½½å¤±è´¥**ï¼šè¯·æ£€æŸ¥çŸ¥è¯†ç©ºé—´ç›®å½•æ˜¯å¦å­˜åœ¨æ–‡æ¡£
        {error_detail}
        
        **è§£å†³æ–¹æ¡ˆï¼š**
        - æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ `embedding.api_key_env` è®¾ç½®
        - ç¡®ä¿ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­æœ‰æœ‰æ•ˆçš„ `DASHSCOPE_API_KEY`
        - å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…ï¼š`pip install llama-index llama-index-embeddings-dashscope`
        - ç¡®ä¿ `rag_source/knowledge_space` ç›®å½•ä¸­æœ‰æ–‡æ¡£æ–‡ä»¶
        - é‡å¯åº”ç”¨ä»¥é‡æ–°åŠ è½½é…ç½®
        
        **å½“å‰å¯ä»¥ä½¿ç”¨"é€šç”¨åŠ©æ‰‹"æ¨¡å¼**ï¼Œè¯¥æ¨¡å¼ä¸ä¾èµ–çŸ¥è¯†åº“ã€‚
        """
        st.error(error_msg)
        full_response = "âš ï¸ çŸ¥è¯†ç©ºé—´æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ä½¿ç”¨é€šç”¨åŠ©æ‰‹æ¨¡å¼ã€‚"
        message_placeholder.markdown(full_response)
        logger.error(f"çŸ¥è¯†ç©ºé—´ç´¢å¼•ä¸º Noneï¼Œæ— æ³•ä½¿ç”¨è¡Œä¸šåŠ©æ‰‹ã€‚é”™è¯¯è¯¦æƒ…ï¼š{error_detail}")
        return full_response, [], "", False, 0.0
    
    logger.info(f"å¼€å§‹å¤„ç†è¡Œä¸šåŠ©æ‰‹æŸ¥è¯¢: prompt={prompt[:50]}...")
    
    # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢æ„å›¾ç©ºé—´
    try:
        intent_text, intent_score, intent_src_nodes = _query_intent_space(
            rag_manager, prompt, k_intent, intent_threshold, show_thinking
        )
        logger.info(f"æ„å›¾ç©ºé—´æŸ¥è¯¢å®Œæˆ: score={intent_score}, has_text={len(intent_text) > 0}")
    except Exception as e:
        logger.error(f"æ„å›¾ç©ºé—´æŸ¥è¯¢å¼‚å¸¸: {e}", exc_info=True)
        intent_text = ""
        intent_score = 0.0
        intent_src_nodes = []
    
    # å¦‚æœæ„å›¾ç©ºé—´ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜ï¼Œç›´æ¥è¿”å›æ„å›¾ç©ºé—´çš„ç­”æ¡ˆ
    use_intent = (intent_score >= intent_threshold) and (len(intent_text.strip()) > 0)
    
    if use_intent:
        # ä½¿ç”¨æ„å›¾ç©ºé—´çš„ç­”æ¡ˆï¼ˆå¿«é€Ÿå“åº”ï¼‰
        full_response = intent_text
        thinking_content_final = ""
        
        # å¦‚æœå¯ç”¨äº†æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œå›ç­”
        if show_thinking and "**å›ç­”ï¼š**" in full_response:
            answer_part, thinking_part = _separate_thinking_and_answer(
                full_response, message_placeholder, thinking_placeholder, show_thinking
            )
            message_placeholder.markdown(answer_part)
            full_response = answer_part
            thinking_content_final = thinking_part
        else:
            message_placeholder.markdown(full_response)
        
        # æ¸…é™¤æµå¼è¾“å‡ºæ—¶çš„thinking_placeholderï¼Œé¿å…ä¸expanderé‡å¤
        if thinking_placeholder:
            thinking_placeholder.empty()
            thinking_placeholder = None
        
        # åœ¨å›ç­”å®Œæˆåï¼Œä½¿ç”¨expanderæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆé»˜è®¤æŠ˜å ï¼‰
        if show_thinking and thinking_content_final:
            with st.expander("ğŸ’­ æŸ¥çœ‹æ€è€ƒè¿‡ç¨‹", expanded=False):
                st.markdown(thinking_content_final)
        
        src_nodes = intent_src_nodes
    else:
        # ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢çŸ¥è¯†ç©ºé—´ï¼Œè·å–æ›´è¯¦ç»†çš„æ–‡æ¡£ä¿¡æ¯
        logger.info(f"æ„å›¾ç©ºé—´ä¸æ»¡è¶³æ¡ä»¶ï¼ŒæŸ¥è¯¢çŸ¥è¯†ç©ºé—´: score={intent_score} < threshold={intent_threshold}")
        try:
            full_response, thinking_content_final, src_nodes = _query_knowledge_space(
                rag_manager, prompt, k_knowledge, message_placeholder, 
                thinking_placeholder, show_thinking
            )
            logger.info(f"çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å®Œæˆ: response_length={len(full_response)}, src_nodes_count={len(src_nodes)}")
        except Exception as e:
            logger.error(f"çŸ¥è¯†ç©ºé—´æŸ¥è¯¢å¼‚å¸¸: {e}", exc_info=True)
            full_response = f"æŠ±æ­‰ï¼ŒæŸ¥è¯¢çŸ¥è¯†ç©ºé—´æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            message_placeholder.markdown(full_response)
            src_nodes = []
    
    # æ„å»ºæ¥æºå­—ç¬¦ä¸²
    sources_str = ",".join([str(getattr(n.node, "metadata", {})) for n in src_nodes])
    
    logger.info(f"è¡Œä¸šåŠ©æ‰‹æŸ¥è¯¢å®Œæˆ: response_length={len(full_response)}, used_intent={use_intent}, intent_score={intent_score}")
    return full_response, src_nodes, sources_str, use_intent, intent_score

